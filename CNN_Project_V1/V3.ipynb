{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 189ms/step - accuracy: 0.6325 - loss: 0.9727 - val_accuracy: 0.8547 - val_loss: 0.3773\n",
      "Epoch 2/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 210ms/step - accuracy: 0.8578 - loss: 0.3842 - val_accuracy: 0.8872 - val_loss: 0.3009\n",
      "Epoch 3/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 186ms/step - accuracy: 0.8834 - loss: 0.3165 - val_accuracy: 0.9034 - val_loss: 0.2595\n",
      "Epoch 4/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 188ms/step - accuracy: 0.8975 - loss: 0.2745 - val_accuracy: 0.9100 - val_loss: 0.2398\n",
      "Epoch 5/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 189ms/step - accuracy: 0.9060 - loss: 0.2541 - val_accuracy: 0.9180 - val_loss: 0.2287\n",
      "Epoch 6/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 184ms/step - accuracy: 0.9132 - loss: 0.2349 - val_accuracy: 0.9198 - val_loss: 0.2277\n",
      "Epoch 7/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 184ms/step - accuracy: 0.9190 - loss: 0.2201 - val_accuracy: 0.9253 - val_loss: 0.2028\n",
      "Epoch 8/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 184ms/step - accuracy: 0.9207 - loss: 0.2111 - val_accuracy: 0.9253 - val_loss: 0.2032\n",
      "Epoch 9/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 184ms/step - accuracy: 0.9243 - loss: 0.2000 - val_accuracy: 0.9200 - val_loss: 0.2221\n",
      "Epoch 10/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 184ms/step - accuracy: 0.9288 - loss: 0.1861 - val_accuracy: 0.9267 - val_loss: 0.2031\n",
      "Epoch 11/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 192ms/step - accuracy: 0.9338 - loss: 0.1796 - val_accuracy: 0.9289 - val_loss: 0.1982\n",
      "Epoch 12/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 184ms/step - accuracy: 0.9381 - loss: 0.1685 - val_accuracy: 0.9297 - val_loss: 0.1992\n",
      "Epoch 13/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 185ms/step - accuracy: 0.9398 - loss: 0.1614 - val_accuracy: 0.9276 - val_loss: 0.1932\n",
      "Epoch 14/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 186ms/step - accuracy: 0.9409 - loss: 0.1570 - val_accuracy: 0.9334 - val_loss: 0.1892\n",
      "Epoch 15/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 185ms/step - accuracy: 0.9429 - loss: 0.1516 - val_accuracy: 0.9308 - val_loss: 0.1920\n",
      "Epoch 16/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 187ms/step - accuracy: 0.9454 - loss: 0.1498 - val_accuracy: 0.9317 - val_loss: 0.1909\n",
      "Epoch 17/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 189ms/step - accuracy: 0.9467 - loss: 0.1404 - val_accuracy: 0.9340 - val_loss: 0.1895\n",
      "Epoch 18/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 184ms/step - accuracy: 0.9511 - loss: 0.1352 - val_accuracy: 0.9336 - val_loss: 0.1991\n",
      "Epoch 19/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 190ms/step - accuracy: 0.9506 - loss: 0.1297 - val_accuracy: 0.9290 - val_loss: 0.2011\n",
      "Epoch 20/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 184ms/step - accuracy: 0.9528 - loss: 0.1265 - val_accuracy: 0.9345 - val_loss: 0.1945\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9346 - loss: 0.2179\n",
      "Test accuracy: 0.9354000091552734\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "# Load and preprocess the Fashion MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# Reshape and normalize the images\n",
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Define the VGG-style model architecture\n",
    "def VGG():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 1)),\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Compile the model\n",
    "model = VGG()\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss=CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_images, train_labels, epochs=20, batch_size=128, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "\n",
    "# Save the results\n",
    "results = {\n",
    "    'test_loss': test_loss,\n",
    "    'test_accuracy': test_acc,\n",
    "    'history': history.history\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('vgg_fashion_mnist_results.json', 'w') as f:\n",
    "    json.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REsnet with FMNSIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m401s\u001b[0m 1s/step - accuracy: 0.7075 - loss: 1.5427 - val_accuracy: 0.4660 - val_loss: 1.4488\n",
      "Epoch 2/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m577s\u001b[0m 1s/step - accuracy: 0.8974 - loss: 0.2785 - val_accuracy: 0.8453 - val_loss: 0.4260\n",
      "Epoch 3/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 615ms/step - accuracy: 0.9198 - loss: 0.2206 - val_accuracy: 0.9079 - val_loss: 0.2532\n",
      "Epoch 4/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 625ms/step - accuracy: 0.9385 - loss: 0.1715 - val_accuracy: 0.9028 - val_loss: 0.3036\n",
      "Epoch 5/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 625ms/step - accuracy: 0.9479 - loss: 0.1432 - val_accuracy: 0.9168 - val_loss: 0.2427\n",
      "Epoch 6/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 630ms/step - accuracy: 0.9542 - loss: 0.1223 - val_accuracy: 0.9193 - val_loss: 0.2401\n",
      "Epoch 7/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 633ms/step - accuracy: 0.9647 - loss: 0.1003 - val_accuracy: 0.9193 - val_loss: 0.2508\n",
      "Epoch 8/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 647ms/step - accuracy: 0.9674 - loss: 0.0878 - val_accuracy: 0.9097 - val_loss: 0.3404\n",
      "Epoch 9/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 621ms/step - accuracy: 0.9772 - loss: 0.0639 - val_accuracy: 0.8770 - val_loss: 0.3718\n",
      "Epoch 10/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 686ms/step - accuracy: 0.9782 - loss: 0.0600 - val_accuracy: 0.9189 - val_loss: 0.3076\n",
      "Epoch 11/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 639ms/step - accuracy: 0.9850 - loss: 0.0417 - val_accuracy: 0.9143 - val_loss: 0.3379\n",
      "Epoch 12/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 624ms/step - accuracy: 0.9855 - loss: 0.0393 - val_accuracy: 0.9223 - val_loss: 0.3120\n",
      "Epoch 13/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 628ms/step - accuracy: 0.9897 - loss: 0.0292 - val_accuracy: 0.9133 - val_loss: 0.4206\n",
      "Epoch 14/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 624ms/step - accuracy: 0.9829 - loss: 0.0506 - val_accuracy: 0.9196 - val_loss: 0.3365\n",
      "Epoch 15/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 627ms/step - accuracy: 0.9890 - loss: 0.0328 - val_accuracy: 0.9188 - val_loss: 0.2908\n",
      "Epoch 16/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 646ms/step - accuracy: 0.9886 - loss: 0.0341 - val_accuracy: 0.9199 - val_loss: 0.3257\n",
      "Epoch 17/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 620ms/step - accuracy: 0.9949 - loss: 0.0148 - val_accuracy: 0.9212 - val_loss: 0.4693\n",
      "Epoch 18/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 626ms/step - accuracy: 0.9927 - loss: 0.0215 - val_accuracy: 0.9165 - val_loss: 0.4404\n",
      "Epoch 19/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 635ms/step - accuracy: 0.9930 - loss: 0.0189 - val_accuracy: 0.9137 - val_loss: 0.4059\n",
      "Epoch 20/20\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 634ms/step - accuracy: 0.9924 - loss: 0.0215 - val_accuracy: 0.9160 - val_loss: 0.4223\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - accuracy: 0.9108 - loss: 0.4551\n",
      "Test accuracy: 0.9100000262260437\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, Add, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "# Load and preprocess the Fashion MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# Reshape and normalize the images\n",
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Define the ResNet model architecture\n",
    "def resnet_block(input_tensor, filters, kernel_size=3, stride=1):\n",
    "    x = Conv2D(filters, kernel_size=kernel_size, strides=stride, padding='same')(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters, kernel_size=kernel_size, strides=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    if stride != 1:\n",
    "        input_tensor = Conv2D(filters, kernel_size=1, strides=stride, padding='same')(input_tensor)\n",
    "        input_tensor = BatchNormalization()(input_tensor)\n",
    "\n",
    "    x = Add()([x, input_tensor])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def ResNet(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(32, kernel_size=3, strides=1, padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = resnet_block(x, 32)\n",
    "    x = resnet_block(x, 32)\n",
    "    x = resnet_block(x, 64, stride=2)\n",
    "    x = resnet_block(x, 64)\n",
    "    x = resnet_block(x, 128, stride=2)\n",
    "    x = resnet_block(x, 128)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs, x)\n",
    "    return model\n",
    "\n",
    "# Compile the model\n",
    "model = ResNet(input_shape=(28, 28, 1), num_classes=10)\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss=CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_images, train_labels, epochs=20, batch_size=128, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "\n",
    "# Save the results\n",
    "results = {\n",
    "    'test_loss': test_loss,\n",
    "    'test_accuracy': test_acc,\n",
    "    'history': history.history\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('resnet_fashion_mnist_results.json', 'w') as f:\n",
    "    json.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LENET WITH FMNSIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.6201 - loss: 1.0662 - val_accuracy: 0.7902 - val_loss: 0.5754\n",
      "Epoch 2/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.7995 - loss: 0.5379 - val_accuracy: 0.8277 - val_loss: 0.4775\n",
      "Epoch 3/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8328 - loss: 0.4569 - val_accuracy: 0.8358 - val_loss: 0.4407\n",
      "Epoch 4/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8501 - loss: 0.4170 - val_accuracy: 0.8506 - val_loss: 0.4134\n",
      "Epoch 5/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8622 - loss: 0.3821 - val_accuracy: 0.8536 - val_loss: 0.4096\n",
      "Epoch 6/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8655 - loss: 0.3640 - val_accuracy: 0.8633 - val_loss: 0.3790\n",
      "Epoch 7/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8727 - loss: 0.3449 - val_accuracy: 0.8637 - val_loss: 0.3649\n",
      "Epoch 8/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8786 - loss: 0.3317 - val_accuracy: 0.8718 - val_loss: 0.3533\n",
      "Epoch 9/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8848 - loss: 0.3185 - val_accuracy: 0.8794 - val_loss: 0.3322\n",
      "Epoch 10/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8897 - loss: 0.3044 - val_accuracy: 0.8805 - val_loss: 0.3264\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8760 - loss: 0.3445\n",
      "Test accuracy: 0.8754000067710876\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, AveragePooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "# Load and preprocess the Fashion MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# Reshape and normalize the images\n",
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Define the LeNet model architecture\n",
    "def LeNet():\n",
    "    model = Sequential([\n",
    "        Conv2D(6, kernel_size=(5, 5), activation='relu', input_shape=(28, 28, 1)),\n",
    "        AveragePooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(16, kernel_size=(5, 5), activation='relu'),\n",
    "        AveragePooling2D(pool_size=(2, 2), ),\n",
    "        Flatten(),\n",
    "        Dense(120, activation='relu'),\n",
    "        Dense(84, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Compile the model\n",
    "model = LeNet()\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss=CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_images, train_labels, epochs=10, batch_size=128, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "\n",
    "# Save the results\n",
    "results = {\n",
    "    'test_loss': test_loss,\n",
    "    'test_accuracy': test_acc,\n",
    "    'history': history.history\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('lenet_fashion_mnist_results.json', 'w') as f:\n",
    "    json.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Googlenet WITH FMNSIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 82ms/step - accuracy: 0.3566 - loss: 1.6248 - val_accuracy: 0.7027 - val_loss: 0.7477\n",
      "Epoch 2/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 75ms/step - accuracy: 0.6992 - loss: 0.7850 - val_accuracy: 0.7717 - val_loss: 0.5970\n",
      "Epoch 3/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 75ms/step - accuracy: 0.7549 - loss: 0.6461 - val_accuracy: 0.8039 - val_loss: 0.5133\n",
      "Epoch 4/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 76ms/step - accuracy: 0.7868 - loss: 0.5808 - val_accuracy: 0.8065 - val_loss: 0.4972\n",
      "Epoch 5/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 79ms/step - accuracy: 0.8136 - loss: 0.5195 - val_accuracy: 0.8372 - val_loss: 0.4319\n",
      "Epoch 6/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 77ms/step - accuracy: 0.8296 - loss: 0.4816 - val_accuracy: 0.8499 - val_loss: 0.3990\n",
      "Epoch 7/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 77ms/step - accuracy: 0.8420 - loss: 0.4467 - val_accuracy: 0.8540 - val_loss: 0.3953\n",
      "Epoch 8/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 77ms/step - accuracy: 0.8545 - loss: 0.4072 - val_accuracy: 0.8583 - val_loss: 0.3873\n",
      "Epoch 9/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 78ms/step - accuracy: 0.8610 - loss: 0.3885 - val_accuracy: 0.8663 - val_loss: 0.3642\n",
      "Epoch 10/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 75ms/step - accuracy: 0.8717 - loss: 0.3692 - val_accuracy: 0.8789 - val_loss: 0.3338\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.8775 - loss: 0.3401\n",
      "Test accuracy: 0.8723000288009644\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, AveragePooling2D, Flatten, Dense, Input, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "# Load and preprocess the Fashion MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# Reshape and normalize the images\n",
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Define the Inception module\n",
    "def inception_module(x, filters):\n",
    "    branch1x1 = Conv2D(filters[0], (1, 1), padding='same', activation='relu')(x)\n",
    "    \n",
    "    branch3x3 = Conv2D(filters[1], (1, 1), padding='same', activation='relu')(x)\n",
    "    branch3x3 = Conv2D(filters[1], (3, 3), padding='same', activation='relu')(branch3x3)\n",
    "    \n",
    "    branch5x5 = Conv2D(filters[2], (1, 1), padding='same', activation='relu')(x)\n",
    "    branch5x5 = Conv2D(filters[2], (5, 5), padding='same', activation='relu')(branch5x5)\n",
    "    \n",
    "    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "    branch_pool = Conv2D(filters[3], (1, 1), padding='same', activation='relu')(branch_pool)\n",
    "    \n",
    "    # Concatenate all the branches along the depth axis\n",
    "    return concatenate([branch1x1, branch3x3, branch5x5, branch_pool], axis=-1)\n",
    "\n",
    "# Define the GoogLeNet model\n",
    "def googlenet(input_shape=(28, 28, 1), num_classes=10):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Initial Convolution Layer\n",
    "    x = Conv2D(64, (7, 7), strides=(2, 2), padding='same', activation='relu')(inputs)\n",
    "    x = AveragePooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "    # First Inception module\n",
    "    x = inception_module(x, filters=(32, 64, 16, 16))\n",
    "    \n",
    "    # Second Inception module\n",
    "    x = inception_module(x, filters=(64, 128, 32, 32))\n",
    "\n",
    "    # Global Average Pooling\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # Fully connected layers\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "    # Output layer\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs, output)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Instantiate the GoogLeNet model\n",
    "model = googlenet(input_shape=(28, 28, 1), num_classes=10)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss=CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_images, train_labels, epochs=10, batch_size=128, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "\n",
    "# Save the results\n",
    "results = {\n",
    "    'test_loss': test_loss,\n",
    "    'test_accuracy': test_acc,\n",
    "    'history': history.history\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('googlenet_fashion_mnist_results.json', 'w') as f:\n",
    "    json.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xceptionnet withFMNSIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 395ms/step - accuracy: 0.6456 - loss: 1.0140 - val_accuracy: 0.1378 - val_loss: 2.7999\n",
      "Epoch 2/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 400ms/step - accuracy: 0.8672 - loss: 0.4010 - val_accuracy: 0.8884 - val_loss: 0.3121\n",
      "Epoch 3/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 345ms/step - accuracy: 0.8948 - loss: 0.3266 - val_accuracy: 0.8934 - val_loss: 0.3064\n",
      "Epoch 4/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 334ms/step - accuracy: 0.9056 - loss: 0.2826 - val_accuracy: 0.8897 - val_loss: 0.3484\n",
      "Epoch 5/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 326ms/step - accuracy: 0.9118 - loss: 0.2633 - val_accuracy: 0.8842 - val_loss: 0.4411\n",
      "Epoch 6/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 339ms/step - accuracy: 0.9209 - loss: 0.2358 - val_accuracy: 0.8972 - val_loss: 0.2824\n",
      "Epoch 7/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 334ms/step - accuracy: 0.9283 - loss: 0.2103 - val_accuracy: 0.8962 - val_loss: 0.2811\n",
      "Epoch 8/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 326ms/step - accuracy: 0.9311 - loss: 0.2078 - val_accuracy: 0.8900 - val_loss: 0.3247\n",
      "Epoch 9/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 327ms/step - accuracy: 0.9392 - loss: 0.1866 - val_accuracy: 0.9043 - val_loss: 0.3290\n",
      "Epoch 10/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 326ms/step - accuracy: 0.9428 - loss: 0.1648 - val_accuracy: 0.8961 - val_loss: 0.3068\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 37ms/step - accuracy: 0.8958 - loss: 0.3342\n",
      "Test accuracy: 0.8953999876976013\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, GlobalAveragePooling2D, Dense, Input, SeparableConv2D, MaxPooling2D, BatchNormalization, Activation, Add\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "# Load and preprocess the Fashion MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# Reshape and normalize the images\n",
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Convert grayscale images to RGB (3 channels)\n",
    "train_images = tf.image.grayscale_to_rgb(tf.convert_to_tensor(train_images))\n",
    "test_images = tf.image.grayscale_to_rgb(tf.convert_to_tensor(test_images))\n",
    "\n",
    "# Resize images to the input size required by Xception (71x71)\n",
    "train_images = tf.image.resize(train_images, [71, 71])\n",
    "test_images = tf.image.resize(test_images, [71, 71])\n",
    "\n",
    "\n",
    "#Define xception block\n",
    "def xception_block(x, filters, strides=(1, 1), shortcut_connection=True):\n",
    "    residual = x\n",
    "\n",
    "    # Separable Conv 1\n",
    "    x = SeparableConv2D(filters, (3, 3), padding='same', strides=strides)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # Separable Conv 2\n",
    "    x = SeparableConv2D(filters, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Shortcut connection\n",
    "    if shortcut_connection:\n",
    "        # Adjust residual path to match dimensions if necessary\n",
    "        if strides != (1, 1) or residual.shape[-1] != filters:\n",
    "            residual = Conv2D(filters, (1, 1), strides=strides, padding='same')(residual)\n",
    "            residual = BatchNormalization()(residual)\n",
    "        x = Add()([x, residual])  # Add the residual connection\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "# Define the Xception CNN model\n",
    "def xception_cnn(input_shape=(71, 71, 3), num_classes=10):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Initial Convolution Layer\n",
    "    x = Conv2D(32, (3, 3), strides=(2, 2), padding='same', activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # MaxPooling Layer\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "    # Xception Blocks\n",
    "    x = xception_block(x, filters=64, shortcut_connection=False)\n",
    "    x = xception_block(x, filters=128, strides=(2, 2))\n",
    "    x = xception_block(x, filters=256, strides=(2, 2))\n",
    "    x = xception_block(x, filters=728, strides=(2, 2))\n",
    "\n",
    "    # Global Average Pooling\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # Fully connected layers\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "    # Output layer\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs, output)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Instantiate the Xception CNN model\n",
    "model = xception_cnn(input_shape=(71, 71, 3), num_classes=10)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss=CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_images, train_labels, epochs=10, batch_size=128, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "\n",
    "# Save the results\n",
    "results = {\n",
    "    'test_loss': test_loss,\n",
    "    'test_accuracy': test_acc,\n",
    "    'history': history.history\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('xception_fashion_mnist_results.json', 'w') as f:\n",
    "    json.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "senet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m394s\u001b[0m 950ms/step - accuracy: 0.6887 - loss: 0.9096 - val_accuracy: 0.4277 - val_loss: 1.8577\n",
      "Epoch 2/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 841ms/step - accuracy: 0.8810 - loss: 0.3670 - val_accuracy: 0.8714 - val_loss: 0.3581\n",
      "Epoch 3/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 821ms/step - accuracy: 0.9012 - loss: 0.3011 - val_accuracy: 0.8757 - val_loss: 0.3598\n",
      "Epoch 4/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 822ms/step - accuracy: 0.9095 - loss: 0.2733 - val_accuracy: 0.9131 - val_loss: 0.2479\n",
      "Epoch 5/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 797ms/step - accuracy: 0.9182 - loss: 0.2396 - val_accuracy: 0.8819 - val_loss: 0.3207\n",
      "Epoch 6/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 810ms/step - accuracy: 0.9243 - loss: 0.2181 - val_accuracy: 0.8727 - val_loss: 0.3530\n",
      "Epoch 7/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 806ms/step - accuracy: 0.9326 - loss: 0.2007 - val_accuracy: 0.9095 - val_loss: 0.2680\n",
      "Epoch 8/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 806ms/step - accuracy: 0.9373 - loss: 0.1808 - val_accuracy: 0.8828 - val_loss: 0.3231\n",
      "Epoch 9/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 798ms/step - accuracy: 0.9455 - loss: 0.1586 - val_accuracy: 0.9029 - val_loss: 0.3709\n",
      "Epoch 10/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 789ms/step - accuracy: 0.9493 - loss: 0.1508 - val_accuracy: 0.9149 - val_loss: 0.2506\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 61ms/step - accuracy: 0.9080 - loss: 0.2790\n",
      "Test accuracy: 0.9092000126838684\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, GlobalAveragePooling2D, Dense, Input, SeparableConv2D, MaxPooling2D, BatchNormalization, Activation, Add, Reshape, Multiply\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "# Load and preprocess the Fashion MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# Reshape and normalize the images\n",
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Convert grayscale images to RGB (3 channels)\n",
    "train_images = tf.image.grayscale_to_rgb(tf.convert_to_tensor(train_images))\n",
    "test_images = tf.image.grayscale_to_rgb(tf.convert_to_tensor(test_images))\n",
    "\n",
    "# Resize images to a larger size for better model flexibility\n",
    "train_images = tf.image.resize(train_images, [71, 71])\n",
    "test_images = tf.image.resize(test_images, [71, 71])\n",
    "\n",
    "# Define the Squeeze-and-Excitation block\n",
    "def se_block(input_tensor, reduction_ratio=16):\n",
    "    filters = input_tensor.shape[-1]  # Number of channels\n",
    "    se = GlobalAveragePooling2D()(input_tensor)\n",
    "    se = Dense(filters // reduction_ratio, activation='relu')(se)\n",
    "    se = Dense(filters, activation='sigmoid')(se)\n",
    "    se = Reshape([1, 1, filters])(se)\n",
    "    return Multiply()([input_tensor, se])\n",
    "\n",
    "# Define a basic convolutional block with an SE block\n",
    "def senet_block(x, filters, strides=(1, 1), shortcut_connection=True):\n",
    "    residual = x\n",
    "\n",
    "    # Convolutional layers\n",
    "    x = Conv2D(filters, (3, 3), padding='same', strides=strides)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Squeeze-and-Excitation\n",
    "    x = se_block(x)\n",
    "\n",
    "    # Shortcut connection\n",
    "    if shortcut_connection:\n",
    "        if strides != (1, 1) or residual.shape[-1] != filters:\n",
    "            residual = Conv2D(filters, (1, 1), strides=strides, padding='same')(residual)\n",
    "            residual = BatchNormalization()(residual)\n",
    "        x = Add()([x, residual])\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "# Define the SENet CNN model\n",
    "def senet_cnn(input_shape=(71, 71, 3), num_classes=10):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Initial Convolution Layer\n",
    "    x = Conv2D(32, (3, 3), strides=(2, 2), padding='same', activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # MaxPooling Layer\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "    # SENet Blocks\n",
    "    x = senet_block(x, filters=64, shortcut_connection=False)\n",
    "    x = senet_block(x, filters=128, strides=(2, 2))\n",
    "    x = senet_block(x, filters=256, strides=(2, 2))\n",
    "    x = senet_block(x, filters=512, strides=(2, 2))\n",
    "\n",
    "    # Global Average Pooling\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # Fully connected layers\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "    # Output layer\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs, output)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Instantiate the SENet CNN model\n",
    "model = senet_cnn(input_shape=(71, 71, 3), num_classes=10)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss=CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_images, train_labels, epochs=10, batch_size=128, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "\n",
    "# Save the results\n",
    "results = {\n",
    "    'test_loss': test_loss,\n",
    "    'test_accuracy': test_acc,\n",
    "    'history': history.history\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('senet_fashion_mnist_results.json', 'w') as f:\n",
    "    json.dump(results, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
